{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alex31y/MortadellAI-hackatlon/blob/main/MortadellAI_1_2_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZV5AXDsK5Wm"
      },
      "source": [
        "**changelog v1.2**:\n",
        "\n",
        "\n",
        "*   Aggiunta la possibilità di far selezionare all'utente il livello di competenza a proposito del documento dato in input\n",
        "*   Ad un errore nella risposta del quiz, viene generata una *lezione personalizzata*\n",
        "*   Raffinamento del prompt per la generazione del corpus:\n",
        ">ora è più prolisso ma può essere ripetitivo / poco scorrevole\n",
        "*   Generazione text to speech di un file .wav del podcast\n",
        ">il modello per la generazione viene fatto girare in \"locale\", rallentando significativamente i tempo di computazione dell'applicazione\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zX5YHIljzvb"
      },
      "source": [
        "**Librerie da installare**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uXFTGYtKw6U"
      },
      "outputs": [],
      "source": [
        "!pip install google-cloud-aiplatform --upgrade --user\n",
        "!pip install gradio PyPDF2\n",
        "!pip install git+https://github.com/suno-ai/bark.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSLUKV1Klctm"
      },
      "source": [
        "**Blocco per il riavvio del runtime, da eseguire solo una volta dopo l'installazione delle librerie sopra**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAayxfESNomi",
        "outputId": "1890a5d0-64eb-4bbd-9fc8-41bd2fd9adb9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# # Automatically restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Oe9luBRMZOm"
      },
      "source": [
        "**Autenticazione**\n",
        "\n",
        "per far girare l'applicazione in locale seguire la seguente guida:\n",
        "\n",
        "https://cloud.google.com/docs/authentication/provide-credentials-adc?hl=it#how-to"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0UFLi6EJKzdD"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import PyPDF2\n",
        "import vertexai\n",
        "from vertexai.preview.language_models import TextGenerationModel\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvbatDlFMV7O"
      },
      "source": [
        "**Configurazione del modello**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sq49rxqMPcX"
      },
      "source": [
        "**LLM DI GOOGLE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2hDjxapAj2M"
      },
      "source": [
        "Sintetizzazione del testo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BKhluKfNLJj_"
      },
      "outputs": [],
      "source": [
        "def riassumi(articolo):\n",
        "  vertexai.init(project=\"gen-mortadellai\", location=\"us-central1\")\n",
        "  parameters = {\n",
        "      \"temperature\": 0.2,\n",
        "      \"max_output_tokens\": 1024,\n",
        "      \"top_p\": 0.95,\n",
        "      \"top_k\": 40\n",
        "  }\n",
        "  model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "  response = model.predict(\n",
        "      f\"\"\"Provide a summary for the following article: Beyond our own products, we think it\\'s important to make it easy, safe and scalable for others to benefit from these advances by building on top of our best models. Next month, we\\'ll start onboarding individual developers, creators and enterprises so they can try our Generative Language API, initially powered by LaMDA with a range of models to follow. Over time, we intend to create a suite of tools and APIs that will make it easy for others to build more innovative applications with AI. Having the necessary compute power to build reliable and trustworthy AI systems is also crucial to startups, and we are excited to help scale these efforts through our Google Cloud partnerships with Cohere, C3.ai and Anthropic, which was just announced last week. Stay tuned for more developer details soon.\n",
        "  Summary: Google is making its AI technology more accessible to developers, creators, and enterprises. Next month, Google will start onboarding developers to try its Generative Language API, which will initially be powered by LaMDA. Over time, Google intends to create a suite of tools and APIs that will make it easy for others to build more innovative applications with AI. Google is also excited to help scale these efforts through its Google Cloud partnerships with Cohere, C3.ai, and Anthropic.\n",
        "\n",
        "  Provide a summary with about two sentences for the following article: The benefits of electricPromptData kitchens go beyond climate impact, starting with speed. The first time I ever cooked on induction (electric) equipment, the biggest surprise was just how incredibly fast it is. In fact, induction boils water twice as fast as traditional gas equipment and is far more efficient — because unlike a flame, electric heat has nowhere to escape. At Bay View, our training programs help Google chefs appreciate and adjust to the new pace of induction. The speed truly opens up whole new ways of cooking.\n",
        "  Summary: Electric kitchens are faster, more efficient, and better for the environment than gas kitchens. Induction cooking is particularly fast, boiling water twice as fast as traditional gas equipment. This speed opens up whole new ways of cooking. Google chefs are trained to appreciate and adjust to the new pace of induction cooking at Bay View.\n",
        "\n",
        "  Provide a summary with about two sentences for the following article: We\\'re also using AI to forecast floods, another extreme weather pattern exacerbated by climate change. We\\'ve already helped communities to predict when floods will hit and how deep the waters will get — in 2021, we sent 115 million flood alert notifications to 23 million people over Google Search and Maps, helping save countless lives. Today, we\\'re sharing that we\\'re now expanding our coverage to more countries in South America (Brazil and Colombia), Sub-Saharan Africa (Burkina Faso, Cameroon, Chad, Democratic Republic of Congo, Ivory Coast, Ghana, Guinea, Malawi, Nigeria, Sierra Leone, Angola, South Sudan, Namibia, Liberia, and South Africa), and South Asia (Sri Lanka). We\\'ve used an AI technique called transfer learning to make it work in areas where there\\'s less data available. We\\'re also announcing the global launch of Google FloodHub, a new platform that displays when and where floods may occur. We\\'ll also be bringing this information to Google Search and Maps in the future to help more people to reach safety in flooding situations.\n",
        "  Summary: Google is using AI to forecast floods in South America, Sub-Saharan Africa, South Asia, and other parts of the world. The AI technique of transfer learning is being used to make it work in areas where there\\'s less data available. Google FloodHub, a new platform that displays when and where floods may occur, has also been launched globally. This information will also be brought to Google Search and Maps in the future to help more people reach safety in flooding situations.\n",
        "\n",
        "  Provide a summary with about two sentences for the following article: In order to learn skiing, you must first be educated on the proper use of the equipment. This includes learning how to properly fit your boot on your foot, understand the different functions of the ski, and bring gloves, goggles etc. Your instructor starts you with one-footed ski drills. Stepping side-to-side, forward-and-backward, making snow angels while keeping your ski flat to the ground, and gliding with the foot not attached to a ski up for several seconds. Then you can put on both skis and get used to doing them with two skis on at once. Next, before going down the hill, you must first learn how to walk on the flat ground and up small hills through two methods, known as side stepping and herringbone. Now it\\'s time to get skiing! For your first attempted run, you will use the skills you just learned on walking up the hill, to go down a small five foot vertical straight run, in which you will naturally stop on the flat ground. This makes you learn the proper athletic stance to balance and get you used to going down the hill in a safe, controlled setting. What do you need next? To be able to stop yourself. Here, your coach will teach you how to turn your skis into a wedge, also commonly referred to as a pizza, by rotating legs inward and pushing out on the heels. Once learned, you practice a gliding wedge down a small hill where you gradually come to a stop on the flat ground thanks to your wedge. Finally, you learn the necessary skill of getting up after falling, which is much easier than it looks, but once learned, a piece of cake.\n",
        "  Summary: Skiing is a great way to enjoy the outdoors and get some exercise. It can be a little daunting at first, but with a little practice, you\\'ll be skiing like a pro in no time.\n",
        "\n",
        "  Provide a summary with about two sentences for the following article: Yellowstone National Park is an American national park located in the western United States, largely in the northwest corner of Wyoming and extending into Montana and Idaho. It was established by the 42nd U.S. Congress with the Yellowstone National Park Protection Act and signed into law by President Ulysses S. Grant on March 1, 1872. Yellowstone was the first national park in the U.S. and is also widely held to be the first national park in the world.The park is known for its wildlife and its many geothermal features, especially the Old Faithful geyser, one of its most popular. While it represents many types of biomes, the subalpine forest is the most abundant. It is part of the South Central Rockies forests ecoregion.\n",
        "  Summary: Yellowstone National Park is the first national park in the United States and the world. It is located in the western United States, largely in the northwest corner of Wyoming and extending into Montana and Idaho. The park is known for its wildlife and its many geothermal features, especially the Old Faithful geyser.\n",
        "\n",
        "  Provide a summary for the following article: {articolo}\n",
        "  Summary:\n",
        "  \"\"\",\n",
        "      **parameters\n",
        "  )\n",
        "  return response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60WHigaUAlIy"
      },
      "source": [
        "Introduzione del podcast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "QJEz0f0JAyHj"
      },
      "outputs": [],
      "source": [
        "def romanza(context, expertise):\n",
        "    print(f\"\\n\\nLivello di expertise: {expertise}\\n\")\n",
        "    vertexai.init(project=\"gen-mortadellai\", location=\"us-central1\")\n",
        "    parameters = {\n",
        "        \"temperature\": 0.2,\n",
        "        \"max_output_tokens\": 1024,\n",
        "        \"top_p\": 0.95,\n",
        "        \"top_k\": 40\n",
        "    }\n",
        "    model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "    response = model.predict(\n",
        "        f\"\"\"Imagine you are the host of a popular podcast called Il postcast di mortadellAI. Your show is known for its captivating storytelling and immersive atmosphere. You are about to dive into a new episode, and you want to engage your audience from the very beginning. The level of expertise of your audience is: {expertise}. You have some context that you would like to share with your listeners. Start with an introduction that entices and hooks your audience, setting the stage for an unforgettable podcast experience. Then continue the story covering all of the input context as if they were {expertise} of the topic. Make sure to incorporate vivid descriptions, evocative language, and a sense of anticipation that will leave your listeners eagerly waiting for what\\'s to come. Remember, your goal is to create an atmosphere of intrigue and captivate your audience\\'s imagination. Finally end with a grand finale\n",
        "    context:\n",
        "    {context}\n",
        "    \"\"\",\n",
        "        **parameters\n",
        "    )\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt03ly94DTL8"
      },
      "source": [
        "Corpo del podcast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dlK0R0DdDVVD"
      },
      "outputs": [],
      "source": [
        "def corpo(facts):\n",
        "    vertexai.init(project=\"gen-mortadellai\", location=\"us-central1\")\n",
        "    parameters = {\n",
        "        \"temperature\": 0.2,\n",
        "        \"max_output_tokens\": 256,\n",
        "        \"top_p\": 0.8,\n",
        "        \"top_k\": 40\n",
        "    }\n",
        "    model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "    response = model.predict(\n",
        "        f\"\"\"don\\'t be repetitive\n",
        "\n",
        "    Summarize the following facts in an engaging format: [input facts]\n",
        "    units and 6,480 people – representing approximately 0.5% of the housing units and 0.5% of the population of\n",
        "    Allegheny County.\n",
        "    Market Characteristics for Each Identified Cluster\n",
        "    Robust Markets\n",
        "    • “A” markets have the highest housing values, experience the largest level of new construction, have the\n",
        "    highest owner occupancy levels, and experience little housing distress (such as residential vacancy and\n",
        "    foreclosure).\n",
        "    • “B” markets have elevated housing values, experience substantial amounts of new construction, have\n",
        "    more renters than owners, and experience little housing distress.\n",
        "    • “C” markets have above average housing values, experience about average levels of new construction,\n",
        "    have the highest levels of owner occupancy, and experience little housing distress.\n",
        "    Steady Markets\n",
        "    • “D” markets have average housing values, experience half the countywide average amount of new\n",
        "    construction, have more renters than owners, experience average levels of foreclosure, and have low\n",
        "    levels of vacant lots and poor or worse condition properties.\n",
        "    • “E” markets have slightly lower than average housing values, experience little new construction, have\n",
        "    high levels of owner occupancy, have above average amounts of vacant land, and about average levels\n",
        "    of foreclosure.\n",
        "    • “F” markets have slightly lower than average housing values, experience slightly above average amounts\n",
        "    of new construction, have more owners than renters, and have high levels of renters with a subsidy.\n",
        "    Transitional Markets\n",
        "    • “G” markets have below average housing values, experience little new construction, have more owners\n",
        "    than renters, and experience above average levels of foreclosure and residential vacant land.\n",
        "    • “H” markets have housing values well below the countywide average, experience little new\n",
        "    construction, have about even numbers of renters and owners, have the highest share of residential\n",
        "    vacant land, and the highest levels of foreclosure.\n",
        "    Stressed Markets\n",
        "    • “I” markets have the second lowest housing values, experience very little new construction, have the\n",
        "    highest share of renters with a subsidy, experience the highest levels of building violations, and have\n",
        "    elevated shares of poor or worse condition properties, vacant residential lot area, and foreclosure.\n",
        "    • “J” markets have the lowest housing values (although there is a substantial amount of variability in\n",
        "    those prices), more renters than homeowners, the highest share of poor or worse condition properties,\n",
        "    and elevated shares of building violations, vacant lots, and foreclosure.\n",
        "    Allegheny County and City of Pittsburgh Market Value Analysis (MVA)\n",
        "    Average Block Group Housing Market Characteristics - 2021 Allegheny County & City of Pittsburgh MVA\n",
        "    Table One: Average Block Group Characteristics for 2021 Allegheny County & City of Pittsburgh MVA\n",
        "    Allegheny County and City of Pittsburgh MVA, 2021\n",
        "    5\n",
        "    Executive Summary Map One: Allegheny County and City of Pittsburgh MVA, 2021\n",
        "    Allegheny County and City of Pittsburgh MVA, 2021\n",
        "    6\n",
        "    Executive Summary Map Two: Allegheny County and City of Pittsburgh MVA, 2021\n",
        "    Allegheny County and City of Pittsburgh MVA, 2021\n",
        "    7\n",
        "    Population, Race, and Ethnicity by 2021 MVA Market Type\n",
        "    Allegheny County and City of Pittsburgh residents most commonly live in Robust market types (about\n",
        "    44% of County residents), with about 30% of residents in Steady markets, 23% living in Transitional\n",
        "    markets, and 4% in Stressed markets. Robust and Steady housing markets have prices near or above the\n",
        "    Countywide average, a mix of highly owner occupied and mixed tenure areas, little housing distress, and\n",
        "    are where the majority of housing investments are occurring. Black residents are more likely to live in\n",
        "    Transitional (45%) and Stressed (21%) markets than Robust (11%) or Steady (22%) markets, however.\n",
        "    Hispanic residents are also more likely to live in Transitional or Stressed markets (32%) than White\n",
        "    residents (21%). Therefore, People of Color are more likely to live in areas with housing challenges that\n",
        "    limit their ability to build wealth through home equity and may not have sufficient access to mortgage\n",
        "    credit.\n",
        "    Figure Two: Population and Race / Ethnicity by 2021 MVA Market Type\n",
        "    Changes Since the 2016 MVAs\n",
        "    Across all market types, sale prices have increased since the 2016 MVAs. In general, market types with\n",
        "    lower sale prices in 2016 had larger price increases (as a percentage) from 2013-2015 to 2017-2019. The\n",
        "    Stressed markets experienced the most dramatic proportionate rise in typical sale prices. While that is\n",
        "    an extraordinary rise, homes in these areas are transacting at prices that are affordable to households at\n",
        "    approximately one-third the 2015-2019 county median income ($61,043). Sale price increases in all\n",
        "    market types outstripped inflation. In the City of Pittsburgh, each market type experienced a twenty\n",
        "    percent or greater sale price increase, including very large increases (both proportionate and raw\n",
        "    difference) in East End of Pittsburgh, Lawrenceville, and on parts of Northside (‘E’ markets in 2016).\n",
        "    A 76 120,578 10.1% 8% 2% 2% 86% 0%\n",
        "    B 113 134,101 11.2% 8% 6% 3% 81% 1%\n",
        "    C 185 267,610 22.3% 3% 2% 1% 92% 1%\n",
        "    D 100 108,679 9.1% 7% 10% 3% 76% 4%\n",
        "    E 196 215,748 18.0% 2% 6% 2%\n",
        "    output: In Allegheny County, there are various housing markets categorized into different clusters. Let\\'s take a closer look at these clusters:\n",
        "\n",
        "    Robust Markets: These markets have the highest housing values, with a lot of new construction and high levels of owner occupancy. They experience minimal housing distress, such as vacancies and foreclosures.\n",
        "\n",
        "    Steady Markets: These markets have average housing values, moderate new construction, and a mix of renters and owners. They experience average levels of foreclosure but have low levels of vacant lots and poor-condition properties.\n",
        "\n",
        "    Transitional Markets: These markets have below-average housing values and little new construction. They have more owners than renters and experience above-average levels of foreclosure and vacant land.\n",
        "\n",
        "    Stressed Markets: These markets have the lowest housing values, little new construction, and a high share of renters with subsidies. They also have elevated levels of building violations, poor-condition properties, vacant lots, and foreclosures.\n",
        "\n",
        "    Now, let\\'s examine the population distribution across these market types:\n",
        "\n",
        "    Robust markets are home to about 44% of Allegheny County residents, followed by Steady markets with around 30%.\n",
        "    Transitional markets house about 23% of the population, while Stressed markets have approximately 4% of the residents.\n",
        "    Black residents are more likely to live in Transitional (45%) and Stressed (21%) markets compared to Robust (11%) or Steady (22%) markets.\n",
        "    Hispanic residents also tend to reside in Transitional or Stressed markets (32%) more than White residents (21%).\n",
        "    These findings suggest that people of color are more likely to face housing challenges, limiting their ability to build wealth through home equity and access mortgage credit.\n",
        "    Furthermore, there have been changes in sale prices since the 2016 Market Value Analysis (MVA):\n",
        "\n",
        "    Across all market types, sale prices have increased, with larger percentage increases in market types that had lower prices in 2016.\n",
        "    Stressed markets experienced the most significant proportionate rise in sale prices, although they still remain affordable for households earning around one-third of the county median income.\n",
        "    In the City of Pittsburgh, all market types experienced a 20% or greater increase in sale prices, including substantial increases in the East End, Lawrenceville, and parts of the Northside.\n",
        "    These findings highlight the dynamics of the housing market in Allegheny County and the disparities that exist among different market types. It\\'s crucial to address housing challenges in Transitional and Stressed markets, particularly for communities of color, to promote equity and access to homeownership opportunities.\n",
        "\n",
        "    Summarize the following facts in an engaging format: [input facts]\n",
        "    {facts}\n",
        "    output:\n",
        "    \"\"\",\n",
        "        **parameters\n",
        "    )\n",
        "    #print(f\"Response from Model: {response.text}\")\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQYMhAQVRodO"
      },
      "source": [
        "Genera le domande col language mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fbHy3Sw7RsFm"
      },
      "outputs": [],
      "source": [
        "def LMQuiz(context):\n",
        "    import vertexai\n",
        "    from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "    vertexai.init(project=\"gen-mortadellai\", location=\"us-central1\")\n",
        "    parameters = {\n",
        "        \"temperature\": 0.2,\n",
        "        \"max_output_tokens\": 256,\n",
        "        \"top_p\": 0.8,\n",
        "        \"top_k\": 40\n",
        "    }\n",
        "    model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "    response = model.predict(\n",
        "        f\"\"\"instructions:\n",
        "    i want you to generate a series of questions about a given [input_text].\n",
        "    Generate questions following the following structure example in three rows:\n",
        "    question:this is an example of a question ?\n",
        "    options:option1, option2, option3, option4\n",
        "    answer:the answer to the question**\n",
        "\n",
        "    [input_text]:\n",
        "    {context}\"\"\",\n",
        "        **parameters\n",
        "    )\n",
        "    print(f\"Response from LMQuiz: {response.text}\")\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spiega la risposta"
      ],
      "metadata": {
        "id": "tIJWdhmktIXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def explain(question, context):\n",
        "    vertexai.init(project=\"gen-mortadellai\", location=\"us-central1\")\n",
        "    parameters = {\n",
        "        \"temperature\": 0.2,\n",
        "        \"max_output_tokens\": 256,\n",
        "        \"top_p\": 0.8,\n",
        "        \"top_k\": 40\n",
        "    }\n",
        "    model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "    response = model.predict(\n",
        "        f\"\"\"answer this question and explain the reasoning:\n",
        "    {question}\n",
        "\n",
        "    Given this context:\n",
        "    {context}\"\"\",\n",
        "        **parameters\n",
        "    )\n",
        "    print(f\"Response from Model: {response.text}\")\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "Op4j0OB8tHrq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PamK7DINV0De"
      },
      "source": [
        "**Pulizia del testo**\n",
        "\n",
        "**Suddivisione in chunks**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Wh72JOlrTpy2"
      },
      "outputs": [],
      "source": [
        "import unicodedata\n",
        "import re\n",
        "\n",
        "def clean_text(input_text):\n",
        "    text = \"\"\n",
        "\n",
        "    for char in input_text:\n",
        "        try:\n",
        "            char.encode('utf-8')\n",
        "            text += char\n",
        "        except UnicodeEncodeError:\n",
        "            pass\n",
        "\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "def text_to_chunks(text, chunk_size = 10000, overlap_percentage=0.1):\n",
        "    chunks = []\n",
        "    overlap_size = int(chunk_size * overlap_percentage)\n",
        "    start = 0\n",
        "    end = chunk_size\n",
        "\n",
        "    while start < len(text):\n",
        "        chunk = text[start:end]\n",
        "        chunks.append(chunk)\n",
        "        start += chunk_size - overlap_size\n",
        "        end = start + chunk_size\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtKEYC7GS0d3"
      },
      "source": [
        "**Sezione relativa al processing del pdf**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOMro9wpb2vt"
      },
      "source": [
        "per la sintetizzazione riga 43"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "FSFC-UbGS5f2"
      },
      "outputs": [],
      "source": [
        "def extract_text(pdf_file):\n",
        "    # Open the PDF file\n",
        "    with open(pdf_file.name, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        num_pages = len(reader.pages)\n",
        "\n",
        "        # Extract text from each page\n",
        "        text = \"\"\n",
        "        for page_number in range(num_pages):\n",
        "            page = reader.pages[page_number]\n",
        "            text += page.extract_text()\n",
        "\n",
        "    return text\n",
        "\n",
        "def ask_question(context):\n",
        "                question = genera_domanda(context)\n",
        "                question_text = question[\"question\"]\n",
        "                options = question[\"options\"]\n",
        "                answer = question[\"answer\"]\n",
        "\n",
        "                prefix = \"question:\"\n",
        "                if question_text.startswith(prefix):\n",
        "                    question_text = question_text[len(prefix):].strip()\n",
        "\n",
        "                question_full = question_text + \"\\n**Possible answers:**\\n\"\n",
        "                for option in options:\n",
        "                    question_full += \"\\n- \" + option\n",
        "\n",
        "                return question_full, answer\n",
        "\n",
        "\n",
        "# Interface function\n",
        "def pdf_to_text(pdf_file, expertise):\n",
        "    text = extract_text(pdf_file)\n",
        "    text = clean_text(text)\n",
        "    chunks = text_to_chunks(text, chunk_size = 3000)\n",
        "    riassunto = \"\"\n",
        "    corpus = \"\"\n",
        "\n",
        "    for chunk in chunks:\n",
        "      riassunto += riassumi(chunk) + '\\n'\n",
        "    intro = romanza(clean_text(riassunto), expertise)\n",
        "    #sintetizza(intro)\n",
        "    ultimoPezzo = intro # colpo di tacco\n",
        "    for chunk in chunks:\n",
        "      ultimoPezzo = corpo(chunk)\n",
        "      corpus += ultimoPezzo + '\\n'\n",
        "    podcast = intro + \"\\n\\nLa registrazione si ferma qui\\n\\n\" + corpus + \"\\n\\nFine corpus\\n\\n\"\n",
        "    return podcast, riassunto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6GGDn04Ogr3"
      },
      "source": [
        "**Sezione relativa al quiz**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_iS1Xa-R2_f"
      },
      "source": [
        "Question: What is the pomegranate's nutritional value? Options:\n",
        "\n",
        "It is a good source of vitamins, minerals, and antioxidants.\n",
        "It is a good source of fiber.\n",
        "It is a good source of protein.\n",
        "It is a good source of carbohydrates. Answer: It is a good source of vitamins, minerals, and antioxidants."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "uncomment line 60 for audio generation"
      ],
      "metadata": {
        "id": "06n2UXda_tWn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "fjSssHQrOlZ3"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def format_question(questions):\n",
        "    if isinstance(questions, dict):\n",
        "        question_texts = []\n",
        "        for question_text in questions.values():\n",
        "            if isinstance(question_text, str):\n",
        "                question_texts.append(question_text)\n",
        "    else:\n",
        "        if isinstance(questions, str):\n",
        "            question_texts = questions.split(\"\\n\\n\")\n",
        "        elif isinstance(questions, list):\n",
        "            question_texts = [text for text in questions if isinstance(text, str)]\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    formatted_questions = []\n",
        "    for question_text in question_texts:\n",
        "        if question_text.strip() != \"\":\n",
        "            question_lines = question_text.strip().split(\"\\n\")\n",
        "            question = {\n",
        "                \"question\": question_lines[0].replace(\"Question: \", \"\"),\n",
        "                \"options\": [option.strip(\"- \") for option in question_lines[2:-1]],\n",
        "                \"answer\": question_lines[-1].replace(\"Answer: \", \"\")\n",
        "            }\n",
        "            formatted_questions.append(question)\n",
        "\n",
        "    return formatted_questions\n",
        "\n",
        "questions = \"\"\n",
        "question = \"\"\n",
        "import random\n",
        "def genera_domanda(context):\n",
        "    global questions\n",
        "    global question\n",
        "    if not questions:\n",
        "        chunks = text_to_chunks(context)\n",
        "        random_chunk = random.choice(chunks)\n",
        "        questions = LMQuiz(random_chunk)\n",
        "        questions = format_question(questions)\n",
        "        #print(f\"New questions generated: {questions}\")\n",
        "\n",
        "    question = questions.pop(0) if questions else None\n",
        "    #formatted_question = format_question(question) if question else None\n",
        "    return question\n",
        "\n",
        "def quiz(answer, podcast):\n",
        "    domanda = question[\"question\"]\n",
        "    correct_answer = question[\"answer\"]\n",
        "\n",
        "    prefix = \"answer:\"\n",
        "    if correct_answer.startswith(prefix):\n",
        "        correct_answer = correct_answer[len(prefix):].strip()\n",
        "\n",
        "    if answer == correct_answer:\n",
        "        return \"risposta esatta\"\n",
        "\n",
        "    spiegazione = explain(domanda, podcast)\n",
        "    print(\"\\n\\n\" + domanda + \"\\n\\n\")\n",
        "    #sintetizza(spiegazione, domanda)\n",
        "    return \"Wrong answer: [\" + answer + \"] You should have said: \" + correct_answer + \"\\ncontext:\\n\" + spiegazione"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyZa-z1u120Z"
      },
      "source": [
        "**Audio text to speech**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIrn2MxlD2pd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "from scipy.io.wavfile import write as write_wav\n",
        "from IPython.display import Audio\n",
        "import nltk  # we'll use this to split into sentences\n",
        "import numpy as np\n",
        "\n",
        "from bark.generation import (\n",
        "    generate_text_semantic,\n",
        "    preload_models,\n",
        ")\n",
        "from bark.api import semantic_to_waveform\n",
        "from bark import generate_audio, SAMPLE_RATE\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8niqZCJLtgG"
      },
      "source": [
        "ci metterà un po'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAm2Z8uIELhs"
      },
      "outputs": [],
      "source": [
        "preload_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ve9xcInV17gp"
      },
      "outputs": [],
      "source": [
        "def sintetizza(podcast, titolo = \"podcast\"):\n",
        "    podcast = podcast.replace(\"\\n\", \" \").strip()\n",
        "    sentences = nltk.sent_tokenize(podcast)\n",
        "\n",
        "    SPEAKER = \"v2/en_speaker_6\"\n",
        "    silence = np.zeros(int(0.25 * SAMPLE_RATE))  # quarter second of silence\n",
        "\n",
        "    pieces = []\n",
        "    for sentence in sentences:\n",
        "        audio_array = generate_audio(sentence, history_prompt=SPEAKER)\n",
        "        pieces += [audio_array, silence.copy()]\n",
        "\n",
        "    concatenated_audio = np.concatenate(pieces)\n",
        "    audio = Audio(data=concatenated_audio, rate=SAMPLE_RATE)  # Create the Audio object\n",
        "\n",
        "    audio_array = np.frombuffer(audio.data, dtype=np.int16)  # Convert bytes to NumPy array\n",
        "    write_wav(titolo, SAMPLE_RATE, audio_array)\n",
        "    print(\"\\n\\noutput.wav salvato in memoria\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sintetizza in locale"
      ],
      "metadata": {
        "id": "0vjjcZk97BhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "podcast = \"The answer to the question What is Interreg Europe? is that it is a European Union programme that funds interregional cooperation projects. The programme is open to public authorities, public law bodies, and private non-profit bodies from all 27 EU member states, Norway, and Switzerland. Projects must be co-financed by the partners, and the co-financing rate is 80% for public bodies and bodies governed by public law from all 27 EU member states and 70% for private non-profit bodies from all 27 EU member states.\"\n",
        "\n",
        "podcast = podcast.replace(\"\\n\", \" \").strip()\n",
        "sentences = nltk.sent_tokenize(podcast)\n",
        "\n",
        "SPEAKER = \"v2/en_speaker_6\"\n",
        "silence = np.zeros(int(0.25 * SAMPLE_RATE))  # quarter second of silence\n",
        "\n",
        "pieces = []\n",
        "for sentence in sentences:\n",
        "    audio_array = generate_audio(sentence, history_prompt=SPEAKER)\n",
        "    pieces += [audio_array, silence.copy()]\n",
        "\n",
        "concatenated_audio = np.concatenate(pieces)\n",
        "audio = Audio(data=concatenated_audio, rate=SAMPLE_RATE)  # Create the Audio object\n",
        "\n",
        "audio_array = np.frombuffer(audio.data, dtype=np.int16)  # Convert bytes to NumPy array\n",
        "write_wav(\"output.wav\", SAMPLE_RATE, audio_array)\n",
        "print(\"\\n\\noutput.wav salvato in memoria\\n\\n\")"
      ],
      "metadata": {
        "id": "hkP9I9gG7BQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw3vaSaVNxtM"
      },
      "source": [
        "**Gradio Web App**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "uncomment line 43 for audio generation"
      ],
      "metadata": {
        "id": "zcROsUaO_Wam"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkOytB5dOb4O"
      },
      "outputs": [],
      "source": [
        "demo = gr.Blocks(theme='freddyaboulton/dracula_revamped')\n",
        "import time\n",
        "import random\n",
        "podcast = \"\"\n",
        "riassunto = \"\"\n",
        "expertise = \"\"\n",
        "\n",
        "with demo:\n",
        "    gr.Markdown(\"MortadellAI's podcast generator\")\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"Generate a podcast\"):\n",
        "\n",
        "            radio = gr.Radio(\n",
        "                [\"Beginner\", \"Intermediate\", \"Expert\"], label=\"Select your level of expertise on the subject\"\n",
        "            )\n",
        "            def change_textbox(radio):\n",
        "                global expertise\n",
        "                expertise = radio\n",
        "\n",
        "            radio.change(fn=change_textbox, inputs=radio)\n",
        "            file_input = gr.File()\n",
        "            upload_button = gr.Button(\"upload\")\n",
        "            text_output = gr.Textbox()\n",
        "\n",
        "        with gr.TabItem(\"Test yourself!\"):\n",
        "            chatbot = gr.Chatbot()\n",
        "            msg = gr.Textbox()\n",
        "            clear = gr.ClearButton([msg, chatbot])\n",
        "\n",
        "            def respond(message, chat_history):\n",
        "                global podcast\n",
        "                global riassunto\n",
        "                bot_message = quiz(message, podcast) + \"\\n\"\n",
        "                question, answer = ask_question(podcast)\n",
        "                chat_history.append((message, bot_message + question + \"\\n\" + answer))\n",
        "                return \"\", chat_history\n",
        "            msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
        "\n",
        "            def start(file_input, chat_history):\n",
        "                global podcast, riassunto\n",
        "                podcast, riassunto = pdf_to_text(file_input, expertise)\n",
        "\n",
        "                #sintetizza(podcast)\n",
        "\n",
        "                podcast += \"\\n\\nRIASSUNTO:\\n\" + riassunto\n",
        "                print(\"\\n\\npodcast:\\n\" + podcast)\n",
        "                question, answer = ask_question(podcast)\n",
        "                chat_history.append((\"answer by copying the choosen answer\", question + \"\\n\" + answer))\n",
        "                #gr.Audio(value=\"output.wav\", source=\"upload\")\n",
        "                return podcast, chat_history\n",
        "\n",
        "\n",
        "    upload_button.click(start, inputs=[file_input, chatbot], outputs=[text_output, chatbot])\n",
        "\n",
        "\n",
        "demo.queue().launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOc9wIP2BjeaZY2HfjIfOeZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}